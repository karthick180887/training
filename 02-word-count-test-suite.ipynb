{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c954aede-fee4-4337-b572-a0ad6766de7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class batchWC():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/Volumes/workspace/default\"\n",
    "\n",
    "    def getRawData(self):\n",
    "        from pyspark.sql.functions import explode, split\n",
    "        lines = (spark.read\n",
    "                    .format(\"text\")\n",
    "                    .option(\"lineSep\", \".\")\n",
    "                    .load(f\"{self.base_data_dir}/data/text\")\n",
    "                )\n",
    "        return lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "    \n",
    "    def getQualityData(self, rawDF):\n",
    "        from pyspark.sql.functions import trim, lower\n",
    "        return ( rawDF.select(lower(trim(rawDF.word)).alias(\"word\"))\n",
    "                        .where(\"word is not null\")\n",
    "                        .where(\"word rlike '[a-z]'\")\n",
    "                )\n",
    "        \n",
    "    def getWordCount(self, qualityDF):\n",
    "        return qualityDF.groupBy(\"word\").count()\n",
    "    \n",
    "    def overwriteWordCount(self, wordCountDF):\n",
    "        ( wordCountDF.write\n",
    "                    .format(\"delta\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .saveAsTable(\"word_count_table\")\n",
    "        )\n",
    "    \n",
    "    def wordCount(self):\n",
    "        print(f\"\\tExecuting Word Count...\", end='')\n",
    "        rawDF = self.getRawData()\n",
    "        qualityDF = self.getQualityData(rawDF)\n",
    "        resultDF = self.getWordCount(qualityDF)\n",
    "        self.overwriteWordCount(resultDF)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a959c1-cbab-450c-a5e9-69e6c0ce9410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class batchWCTestSuite():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/Volumes/workspace/default\"\n",
    "\n",
    "    def cleanTests(self):\n",
    "        print(f\"Starting Cleanup...\", end='')\n",
    "        spark.sql(\"drop table if exists word_count_table\")\n",
    "        dbutils.fs.rm(\"/Volumes/workspace/default/word_count_table\", True)\n",
    "\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/checkpoint\", True)\n",
    "        dbutils.fs.rm(f\"{self.base_data_dir}/data\", True)\n",
    "\n",
    "        dbutils.fs.mkdirs(f\"{self.base_data_dir}/data\")\n",
    "        print(\"Done\\n\")\n",
    "\n",
    "    def ingestData(self, itr):\n",
    "        print(f\"\\tStarting Ingestion...\", end='')\n",
    "        dbutils.fs.cp(f\"{self.base_data_dir}/datasets/text/text_data_{itr}.txt\", f\"{self.base_data_dir}/data/text/text_data_{itr}.txt\")\n",
    "        print(\"Done\")\n",
    "\n",
    "    def assertResult(self, expected_count):\n",
    "        print(f\"\\tStarting validation...\", end='')\n",
    "        actual_count = spark.sql(\"select sum(count) from word_count_table where substr(word, 1, 1) == 's'\").collect()[0][0]\n",
    "        assert expected_count == actual_count, f\"Test failed! actual count is {actual_count}\"\n",
    "        print(\"Done\")\n",
    "\n",
    "    def runTests(self):\n",
    "        self.cleanTests()\n",
    "        wc = batchWC()\n",
    "\n",
    "        print(\"Testing first iteration of batch word count...\") \n",
    "        self.ingestData(1)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(25)\n",
    "        print(\"First iteration of batch word count completed.\\n\")\n",
    "\n",
    "        print(\"Testing second iteration of batch word count...\") \n",
    "        self.ingestData(2)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(32)\n",
    "        print(\"Second iteration of batch word count completed.\\n\") \n",
    "\n",
    "        print(\"Testing third iteration of batch word count...\") \n",
    "        self.ingestData(3)\n",
    "        wc.wordCount()\n",
    "        self.assertResult(37)\n",
    "        print(\"Third iteration of batch word count completed.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7884f251-2aa1-44e9-a704-408ea1256c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Cleanup...Done\n\nTesting first iteration of batch word count...\n\tStarting Ingestion...Done\n\tExecuting Word Count...Done\n\tStarting validation...Done\nFirst iteration of batch word count completed.\n\nTesting second iteration of batch word count...\n\tStarting Ingestion...Done\n\tExecuting Word Count...Done\n\tStarting validation...Done\nSecond iteration of batch word count completed.\n\nTesting third iteration of batch word count...\n\tStarting Ingestion...Done\n\tExecuting Word Count...Done\n\tStarting validation...Done\nThird iteration of batch word count completed.\n\n"
     ]
    }
   ],
   "source": [
    "bwcTS = batchWCTestSuite()\n",
    "bwcTS.runTests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90b007f9-5219-4d9a-9f5b-73fe8a00597e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----------+\n|database|       tableName|isTemporary|\n+--------+----------------+-----------+\n| default|word_count_table|      false|\n+--------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa390fd5-f0d3-4503-be76-29552b975715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(*)|\n+--------+\n|     146|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from word_count_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c935e0d-28c5-46da-9b81-ce26cbd97c80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n|             word|count|\n+-----------------+-----+\n|            about|    1|\n|          without|    2|\n|           reason|    1|\n|           static|    1|\n|                a|    8|\n|      computation|    3|\n|       event-time|    1|\n|              use|    1|\n|             logs|    1|\n|               it|    1|\n|            spark|    4|\n|  stream-to-batch|    1|\n|        continues|    1|\n|            fast,|    1|\n|           arrive|    1|\n|                r|    1|\n|              can|    3|\n|            final|    1|\n|       guarantees|    3|\n|               to|    6|\n|               in|    4|\n|            would|    1|\n|             will|    2|\n|           system|    1|\n|          ensures|    1|\n|           engine|    4|\n|           python|    1|\n|               or|    1|\n|    checkpointing|    1|\n|             take|    1|\n|         scalable|    1|\n|           joins,|    1|\n|         finally,|    1|\n|     exactly-once|    3|\n|           stream|    2|\n|dataset/dataframe|    2|\n|              the|   15|\n|               of|    3|\n|            built|    1|\n|       end-to-end|    4|\n|      write-ahead|    1|\n|         executed|    1|\n|            java,|    1|\n|       structured|    4|\n|               is|    2|\n|         provides|    1|\n|              and|    7|\n|  fault-tolerance|    2|\n|           result|    1|\n|              way|    1|\n|  fault-tolerant,|    1|\n|          running|    1|\n|    incrementally|    1|\n|           scala,|    1|\n|             user|    1|\n|             care|    1|\n|            batch|    2|\n|               as|    6|\n|             your|    3|\n|        streaming|    9|\n|   fault-tolerant|    1|\n|        optimized|    1|\n|       processing|    6|\n|         windows,|    1|\n|             same|    2|\n|        scalable,|    1|\n|          through|    2|\n|              you|    5|\n|          express|    3|\n|               on|    4|\n|              api|    1|\n|    aggregations,|    1|\n|           short,|    1|\n|              sql|    3|\n|           having|    1|\n|     continuously|    1|\n|             data|    3|\n|         updating|    1|\n|              etc|    1|\n|          engine,|    1|\n|               by|    1|\n|               we|    3|\n|      millisecond|    1|\n|            using|    2|\n|             able|    1|\n|    at-least-once|    1|\n|        processes|    1|\n|       introduced|    1|\n|              low|    2|\n|              are|    3|\n|         changing|    1|\n|               be|    1|\n|              new|    1|\n|      processing,|    1|\n|            small|    1|\n|             mode|    2|\n|           called|    1|\n|     requirements|    1|\n|           choose|    1|\n|        latencies|    2|\n|          queries|    1|\n|            which|    2|\n|       operations|    1|\n|      low-latency|    1|\n|             jobs|    1|\n|        processed|    1|\n|             with|    2|\n|      internally,|    1|\n|             have|    1|\n|         queries,|    1|\n|            since|    1|\n|     milliseconds|    1|\n|         default,|    1|\n|           series|    1|\n|      micro-batch|    2|\n|       continuous|    2|\n|         however,|    1|\n|          streams|    1|\n|          thereby|    1|\n|            based|    1|\n|        achieving|    1|\n|          achieve|    1|\n|      application|    1|\n|            model|    2|\n|           guide,|    1|\n|             then|    1|\n|           simple|    1|\n|            count|    1|\n|            going|    2|\n|           mostly|    1|\n|          discuss|    1|\n|             walk|    1|\n|           first,|    1|\n|            later|    1|\n|      programming|    1|\n|             word|    1|\n|          default|    1|\n|         concepts|    1|\n|           model,|    1|\n|          explain|    1|\n|            start|    1|\n|            query|    1|\n|          example|    1|\n|            let’s|    1|\n|             apis|    1|\n|             this|    1|\n+-----------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from word_count_table\").show(146)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4441ed-bb85-4f49-86a6-49af759e69b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-word-count-test-suite",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}