{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77ba019-4f78-49e3-abdf-60ba6d296cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea8f6822-00df-4d7d-a62f-35a65b7aa1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/17 05:16:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WindowFunctionsExample\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6658be6-7713-40eb-90b4-4976f62d468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+\n",
      "| id|employee|amount|department|      date|\n",
      "+---+--------+------+----------+----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|\n",
      "|  2|     Bob|   150|         A|2025-09-11|\n",
      "|  3| Charlie|   200|         B|2025-09-12|\n",
      "|  4|   Alice|   250|         A|2025-09-13|\n",
      "|  5|     Bob|   300|         A|2025-09-14|\n",
      "|  6| Charlie|   150|         B|2025-09-15|\n",
      "|  7|   Alice|   300|         A|2025-09-16|\n",
      "|  8|     Bob|   200|         A|2025-09-17|\n",
      "|  9| Charlie|   250|         B|2025-09-18|\n",
      "+---+--------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header='True',inferSchema='True').csv(\"/home/iceberg/notebooks/data/data.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a391a8a-f831-4afa-b32e-0e1b2be39f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,row_number,rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b445f2-b772-482b-a0a1-44f10ac936f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"employee\").orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "869f3ba3-90fa-4f54-bf7a-04c06e75aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpecAgg = Window.partitionBy(\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdfbcfdb-4738-4d9a-aad2-76d3d557e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+----------+\n",
      "| id|employee|amount|department|      date|row number|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|         1|\n",
      "|  4|   Alice|   250|         A|2025-09-13|         2|\n",
      "|  7|   Alice|   300|         A|2025-09-16|         3|\n",
      "|  2|     Bob|   150|         A|2025-09-11|         1|\n",
      "|  5|     Bob|   300|         A|2025-09-14|         2|\n",
      "|  8|     Bob|   200|         A|2025-09-17|         3|\n",
      "|  3| Charlie|   200|         B|2025-09-12|         1|\n",
      "|  6| Charlie|   150|         B|2025-09-15|         2|\n",
      "|  9| Charlie|   250|         B|2025-09-18|         3|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row number\",row_number().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579c5361-d52a-4992-8732-d1d97a2f03ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+----+\n",
      "| id|employee|amount|department|      date|rank|\n",
      "+---+--------+------+----------+----------+----+\n",
      "|  1|   Alice|   100|         A|2025-09-10|   1|\n",
      "|  4|   Alice|   250|         A|2025-09-13|   2|\n",
      "|  7|   Alice|   300|         A|2025-09-16|   3|\n",
      "|  2|     Bob|   150|         A|2025-09-11|   1|\n",
      "|  5|     Bob|   300|         A|2025-09-14|   2|\n",
      "|  8|     Bob|   200|         A|2025-09-17|   3|\n",
      "|  3| Charlie|   200|         B|2025-09-12|   1|\n",
      "|  6| Charlie|   150|         B|2025-09-15|   2|\n",
      "|  9| Charlie|   250|         B|2025-09-18|   3|\n",
      "+---+--------+------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09215f5b-e333-48dc-b3f6-dfe29fdd01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+----------+\n",
      "| id|employee|amount|department|      date|Dense Rank|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|         1|\n",
      "|  4|   Alice|   250|         A|2025-09-13|         2|\n",
      "|  7|   Alice|   300|         A|2025-09-16|         3|\n",
      "|  2|     Bob|   150|         A|2025-09-11|         1|\n",
      "|  5|     Bob|   300|         A|2025-09-14|         2|\n",
      "|  8|     Bob|   200|         A|2025-09-17|         3|\n",
      "|  3| Charlie|   200|         B|2025-09-12|         1|\n",
      "|  6| Charlie|   150|         B|2025-09-15|         2|\n",
      "|  9| Charlie|   250|         B|2025-09-18|         3|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"Dense Rank\",dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d084b283-08ee-438a-813f-310e4b084c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+-----+\n",
      "| id|employee|amount|department|      date|ntile|\n",
      "+---+--------+------+----------+----------+-----+\n",
      "|  1|   Alice|   100|         A|2025-09-10|    1|\n",
      "|  4|   Alice|   250|         A|2025-09-13|    1|\n",
      "|  7|   Alice|   300|         A|2025-09-16|    2|\n",
      "|  2|     Bob|   150|         A|2025-09-11|    1|\n",
      "|  5|     Bob|   300|         A|2025-09-14|    1|\n",
      "|  8|     Bob|   200|         A|2025-09-17|    2|\n",
      "|  3| Charlie|   200|         B|2025-09-12|    1|\n",
      "|  6| Charlie|   150|         B|2025-09-15|    1|\n",
      "|  9| Charlie|   250|         B|2025-09-18|    2|\n",
      "+---+--------+------+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ad6e396-dc23-4162-bcdc-b6b39a4b123a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+-----------+\n",
      "| id|employee|amount|department|      date|running_sum|\n",
      "+---+--------+------+----------+----------+-----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|        100|\n",
      "|  4|   Alice|   250|         A|2025-09-13|        350|\n",
      "|  7|   Alice|   300|         A|2025-09-16|        650|\n",
      "|  2|     Bob|   150|         A|2025-09-11|        150|\n",
      "|  5|     Bob|   300|         A|2025-09-14|        450|\n",
      "|  8|     Bob|   200|         A|2025-09-17|        650|\n",
      "|  3| Charlie|   200|         B|2025-09-12|        200|\n",
      "|  6| Charlie|   150|         B|2025-09-15|        350|\n",
      "|  9| Charlie|   250|         B|2025-09-18|        600|\n",
      "+---+--------+------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.withColumn(\"running_sum\",sum(\"amount\").over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef9485b7-4d73-4143-a071-d4d0f8a3a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+------------------+\n",
      "| id|employee|amount|department|      date|       running_avg|\n",
      "+---+--------+------+----------+----------+------------------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|             100.0|\n",
      "|  4|   Alice|   250|         A|2025-09-13|             175.0|\n",
      "|  7|   Alice|   300|         A|2025-09-16|216.66666666666666|\n",
      "|  2|     Bob|   150|         A|2025-09-11|             150.0|\n",
      "|  5|     Bob|   300|         A|2025-09-14|             225.0|\n",
      "|  8|     Bob|   200|         A|2025-09-17|216.66666666666666|\n",
      "|  3| Charlie|   200|         B|2025-09-12|             200.0|\n",
      "|  6| Charlie|   150|         B|2025-09-15|             175.0|\n",
      "|  9| Charlie|   250|         B|2025-09-18|             200.0|\n",
      "+---+--------+------+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "df.withColumn(\"running_avg\",avg(\"amount\").over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "106d00e2-7609-4e6a-a770-959b3c4ee66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+----------+\n",
      "| id|employee|amount|department|      date|min_amount|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|       100|\n",
      "|  4|   Alice|   250|         A|2025-09-13|       100|\n",
      "|  7|   Alice|   300|         A|2025-09-16|       100|\n",
      "|  2|     Bob|   150|         A|2025-09-11|       150|\n",
      "|  5|     Bob|   300|         A|2025-09-14|       150|\n",
      "|  8|     Bob|   200|         A|2025-09-17|       150|\n",
      "|  3| Charlie|   200|         B|2025-09-12|       150|\n",
      "|  6| Charlie|   150|         B|2025-09-15|       150|\n",
      "|  9| Charlie|   250|         B|2025-09-18|       150|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "df.withColumn(\"min_amount\",min(\"amount\").over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7edcea8b-aa1b-4237-a016-20ee43957205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+----------+\n",
      "| id|employee|amount|department|      date|max_amount|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|       300|\n",
      "|  4|   Alice|   250|         A|2025-09-13|       300|\n",
      "|  7|   Alice|   300|         A|2025-09-16|       300|\n",
      "|  2|     Bob|   150|         A|2025-09-11|       300|\n",
      "|  5|     Bob|   300|         A|2025-09-14|       300|\n",
      "|  8|     Bob|   200|         A|2025-09-17|       300|\n",
      "|  3| Charlie|   200|         B|2025-09-12|       250|\n",
      "|  6| Charlie|   150|         B|2025-09-15|       250|\n",
      "|  9| Charlie|   250|         B|2025-09-18|       250|\n",
      "+---+--------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df.withColumn(\"max_amount\",max(\"amount\").over(windowSpecAgg)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d902c3c-9e03-4919-bb19-b66f1b469f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+----------+----------+-----------+\n",
      "| id|employee|amount|department|      date|lead_amount|\n",
      "+---+--------+------+----------+----------+-----------+\n",
      "|  1|   Alice|   100|         A|2025-09-10|        250|\n",
      "|  4|   Alice|   250|         A|2025-09-13|        300|\n",
      "|  7|   Alice|   300|         A|2025-09-16|       NULL|\n",
      "|  2|     Bob|   150|         A|2025-09-11|        300|\n",
      "|  5|     Bob|   300|         A|2025-09-14|        200|\n",
      "|  8|     Bob|   200|         A|2025-09-17|       NULL|\n",
      "|  3| Charlie|   200|         B|2025-09-12|        150|\n",
      "|  6| Charlie|   150|         B|2025-09-15|        250|\n",
      "|  9| Charlie|   250|         B|2025-09-18|       NULL|\n",
      "+---+--------+------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"lead_amount\",lead(\"amount\", 1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c104176-8845-45ab-beb3-382e59ffa680",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lead\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlag_amount\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mlag\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mover(windowSpec))\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lag' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"lag_amount\",lag(\"amount\", 1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd0e5a-dec1-4406-a5c4-5b3d3d4eccb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
